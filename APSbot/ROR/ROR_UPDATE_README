1. Download the latest ROR release to ROR directory (for example):

  mv ~/Downloads/v1.1-2022-06-16-ror-data.json .

2. Link to ror-data.json:

  rm ror-data.json
  ln -s v1.1-2022-06-16-ror-data.json ror-data.json

3. Generate ror_metadata.csv file from json:

  python3 extract_ror_metadata.py 

4. Get current Wikidata ROR statements (generates wikidata_ror.csv):

  python3 fetch_wikidata_ror_entries.py 

5. Compare ROR metadata and Wikidata ROR entries (generates ror_wd_diffs.csv):

  python3 comparisons.py 

6. Create new ROR release in Wikidata - see Q111395396 for example

7. Update qs_add_ror.py with new ROR release ID

8. Filter ror_wd_diffs to get ROR ID's with Wikidata links not found in WD:
     grep "missing in" ror_wd_diffs.csv > unlinked_vxx.csv
     (and clean up that file copied to ror_wikidata.csv)

9. Run quickstatements generated by python script to add these links to WD:
  python3 qs_add_ror.py > qs_tmp

10. Filter ror_wd_diffs to get ROR ID's with mismatches:
     grep "differs" ror_wd_diffs.csv > mismatches_vxx
    
11. Then compare with previous mismatches, looking at only new ones:
     sort mismatches_v1.0 > tmp
     sort mismatches_v1.1 > tmp2
     diff tmp tmp2 | grep '>' > tmp3
    and look at the two qids to see if something needs to be fixed

12. Re-run #4 above and then run 
     python3 ror_md_for_or.py
   to generate ror_or_metadata.csv for use with OpenRefine

13.
   (a) For each, filter those with no wikidata qid
   (b) Check for ones with a wikipedia URL, if specific enough, link up!
   (c) Match purely on name, see how it does...
   (d) Match on name and country
   (e) Match on name and url
   (f) Match on name and ISNI ?
   (g) for each, export csv of matches to run APSbot on

(14) From "create" subdirectory, run APSbot_ror_create on remaining entries, after updating the release qid in the ror_release_qid file
